{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03d9f39a",
   "metadata": {},
   "source": [
    "# Task 2 - Model Building and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "395a585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import ipaddress\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "sys.path.append(os.path.abspath(\"../Model_Training\"))\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, average_precision_score, confusion_matrix, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56166c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47552ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model_Training.preprocessing import load_data, preprocess_data\n",
    "from Model_Training.model_utils import train_model, evaluate_model, save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d661e448",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "        \"creditcard\": {\n",
    "            \"path\": \"Data/creditcard.csv\",\n",
    "            \"label_col\": \"Class\",\n",
    "            \"model_paths\": {\n",
    "                \"logistic\": \"Models/creditcard_logistic.pkl\",\n",
    "                \"ensemble\": \"Models/creditcard_rf.pkl\",\n",
    "            },\n",
    "            \"report_paths\": {\n",
    "                \"logistic\": \"Outputs/Evaluation_Reports/creditcard_logistic_report.txt\",\n",
    "                \"ensemble\": \"Outputs/Evaluation_Reports/creditcard_rf_report.txt\",\n",
    "            }\n",
    "        },\n",
    "        \"fraud_data\": {\n",
    "            \"path\": \"Data/fraud_data.csv\",\n",
    "            \"label_col\": \"class\",\n",
    "            \"model_paths\": {\n",
    "                \"logistic\": \"Models/frauddata_logistic.pkl\",\n",
    "                \"ensemble\": \"Models/frauddata_rf.pkl\",\n",
    "            },\n",
    "            \"report_paths\": {\n",
    "                \"logistic\": \"Outputs/Evaluation_Reports/frauddata_logistic_report.txt\",\n",
    "                \"ensemble\": \"Outputs/Evaluation_Reports/frauddata_rf_report.txt\",\n",
    "            }\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d8bdc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing creditcard ---\n",
      "Dataset: creditcard\n",
      "Model: Logistic Regression\n",
      "F1 Score: 0.9489\n",
      "AUC-PR: 0.9912\n",
      "Confusion Matrix:\n",
      "[[55361  1389]\n",
      " [ 4289 52687]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95     56750\n",
      "           1       0.97      0.92      0.95     56976\n",
      "\n",
      "    accuracy                           0.95    113726\n",
      "   macro avg       0.95      0.95      0.95    113726\n",
      "weighted avg       0.95      0.95      0.95    113726\n",
      "\n",
      "\n",
      "Dataset: creditcard\n",
      "Model: Random Forest\n",
      "F1 Score: 0.9999\n",
      "AUC-PR: 1.0000\n",
      "Confusion Matrix:\n",
      "[[56738    12]\n",
      " [    0 56976]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56750\n",
      "           1       1.00      1.00      1.00     56976\n",
      "\n",
      "    accuracy                           1.00    113726\n",
      "   macro avg       1.00      1.00      1.00    113726\n",
      "weighted avg       1.00      1.00      1.00    113726\n",
      "\n",
      "\n",
      "\n",
      "--- Processing fraud_data ---\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '2015-02-24 22:55:49'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_17736\\2108176190.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      2\u001b[39m     print(\u001b[33mf\"\\n--- Processing {dataset_name} ---\"\u001b[39m)\n\u001b[32m      3\u001b[39m     df = load_data(config[\u001b[33m\"path\"\u001b[39m])\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m df \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m      5\u001b[39m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     X_train, X_test, y_train, y_test = preprocess_data(df, config[\u001b[33m\"label_col\"\u001b[39m])\n\u001b[32m      7\u001b[39m \n\u001b[32m      8\u001b[39m         \u001b[38;5;66;03m# Logistic Regression\u001b[39;00m\n\u001b[32m      9\u001b[39m     logistic_model = train_model(X_train, y_train, model_type=\u001b[33m\"logistic\"\u001b[39m)\n",
      "\u001b[32md:\\KAIM 5,6,7\\Week-8 & 9\\Fraud-detection-for-Ecommerce-Bank\\Model_Training\\preprocessing.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(df, label_col, scale, balance)\u001b[39m\n\u001b[32m     20\u001b[39m     y = df[label_col]\n\u001b[32m     21\u001b[39m \n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m scale:\n\u001b[32m     23\u001b[39m         scaler = StandardScaler()\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m         X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n\u001b[32m     25\u001b[39m \n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m balance:\n\u001b[32m     27\u001b[39m         sm = SMOTE(random_state=\u001b[32m42\u001b[39m)\n",
      "\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\_set_output.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m     @wraps(f)\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m wrapped(self, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m         data_to_wrap = f(self, X, *args, **kwargs)\n\u001b[32m    320\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m isinstance(data_to_wrap, tuple):\n\u001b[32m    321\u001b[39m             \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m             return_tuple = (\n",
      "\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    914\u001b[39m                 )\n\u001b[32m    915\u001b[39m \n\u001b[32m    916\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m             \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.fit(X, **fit_params).transform(X)\n\u001b[32m    919\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    920\u001b[39m             \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m    921\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.fit(X, y, **fit_params).transform(X)\n",
      "\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    890\u001b[39m             Fitted scaler.\n\u001b[32m    891\u001b[39m         \"\"\"\n\u001b[32m    892\u001b[39m         \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[32m    893\u001b[39m         self._reset()\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self.partial_fit(X, y, sample_weight)\n",
      "\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1385\u001b[39m                 skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m                     prefer_skip_nested_validation \u001b[38;5;28;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m                 )\n\u001b[32m   1388\u001b[39m             ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    926\u001b[39m         self : object\n\u001b[32m    927\u001b[39m             Fitted scaler.\n\u001b[32m    928\u001b[39m         \"\"\"\n\u001b[32m    929\u001b[39m         first_call = \u001b[38;5;28;01mnot\u001b[39;00m hasattr(self, \u001b[33m\"n_samples_seen_\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         X = validate_data(\n\u001b[32m    931\u001b[39m             self,\n\u001b[32m    932\u001b[39m             X,\n\u001b[32m    933\u001b[39m             accept_sparse=(\u001b[33m\"csr\"\u001b[39m, \u001b[33m\"csc\"\u001b[39m),\n",
      "\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2940\u001b[39m             out = y\n\u001b[32m   2941\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2942\u001b[39m             out = X, y\n\u001b[32m   2943\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m no_val_X \u001b[38;5;28;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2944\u001b[39m         out = check_array(X, input_name=\u001b[33m\"X\"\u001b[39m, **check_params)\n\u001b[32m   2945\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2946\u001b[39m         out = _check_y(y, **check_params)\n\u001b[32m   2947\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1052\u001b[39m                         )\n\u001b[32m   1053\u001b[39m                     array = xp.astype(array, dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1054\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1055\u001b[39m                     array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[32m   1057\u001b[39m                 raise ValueError(\n\u001b[32m   1058\u001b[39m                     \u001b[33m\"Complex data not supported\\n{}\\n\"\u001b[39m.format(array)\n\u001b[32m   1059\u001b[39m                 ) from complex_warning\n",
      "\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\_array_api.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(array, dtype, order, copy, xp, device)\u001b[39m\n\u001b[32m    835\u001b[39m         \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[32m    836\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    837\u001b[39m             array = numpy.array(array, order=order, dtype=dtype)\n\u001b[32m    838\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m839\u001b[39m             array = numpy.asarray(array, order=order, dtype=dtype)\n\u001b[32m    840\u001b[39m \n\u001b[32m    841\u001b[39m         \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[32m    842\u001b[39m         \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n",
      "\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, dtype, copy)\u001b[39m\n\u001b[32m   2164\u001b[39m             )\n\u001b[32m   2165\u001b[39m         values = self._values\n\u001b[32m   2166\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2167\u001b[39m             \u001b[38;5;66;03m# Note: branch avoids `copy=None` for NumPy 1.x support\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2168\u001b[39m             arr = np.asarray(values, dtype=dtype)\n\u001b[32m   2169\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2170\u001b[39m             arr = np.array(values, dtype=dtype, copy=copy)\n\u001b[32m   2171\u001b[39m \n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: '2015-02-24 22:55:49'"
     ]
    }
   ],
   "source": [
    "for dataset_name, config in datasets.items():\n",
    "    print(f\"\\n--- Processing {dataset_name} ---\")\n",
    "    df = load_data(config[\"path\"])\n",
    "    if df is None:\n",
    "            continue\n",
    "    X_train, X_test, y_train, y_test = preprocess_data(df, config[\"label_col\"])\n",
    "\n",
    "        # Logistic Regression\n",
    "    logistic_model = train_model(X_train, y_train, model_type=\"logistic\")\n",
    "    save_model(logistic_model, config[\"model_paths\"][\"logistic\"])\n",
    "    evaluate_model(logistic_model, X_test, y_test, dataset_name, \"Logistic Regression\", config[\"report_paths\"][\"logistic\"])\n",
    "\n",
    "    # Random Forest (Ensemble)\n",
    "    rf_model = train_model(X_train, y_train, model_type=\"random_forest\")\n",
    "    save_model(rf_model, config[\"model_paths\"][\"ensemble\"])\n",
    "    evaluate_model(rf_model, X_test, y_test, dataset_name, \"Random Forest\", config[\"report_paths\"][\"ensemble\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
